---
apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: qwen3-8b-disagg-sn  
  namespace: default
spec:
  backendFramework: vllm
  pvcs:
    - name: model-cache
      create: false
  services:
    Frontend:
      dynamoNamespace: qwen3-8b-disagg-sn
      componentType: frontend
      replicas: 1
      resources:
        requests:
          cpu: "1"
          memory: "2Gi"
        limits:
          cpu: "1"
          memory: "2Gi"
      extraPodSpec:
        mainContainer:
          startupProbe:
            httpGet:
              path: /health
              port: 8000
            periodSeconds: 10
            timeoutSeconds: 1800
            failureThreshold: 60
          command:
            - /bin/sh
            - -c
          args:
            - "python3 -m dynamo.frontend --http-port 8000 --router-mode kv"
          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.6.1
          workingDir: /workspace/examples/backends/vllm
      volumeMounts:
        - name: model-cache
          mountPoint: /opt/models
      envs:
        - name: HF_HOME
          value: /opt/models
    VllmDecodeWorker:
      dynamoNamespace: qwen3-8b-disagg-sn
      envFromSecret: hf-token
      componentType: worker
      subComponentType: decode
      volumeMounts:
        - name: model-cache
          mountPoint: /opt/models
      sharedMemory:
        size: 80Gi
      replicas: 1
      resources:
        requests:
          cpu: "8"
          memory: "16Gi"
          gpu: "1"
        limits:
          cpu: "8"
          memory: "16Gi"
          gpu: "1"
      extraPodSpec:
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        affinity:
          nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: nvidia.com/gpu.present
                        operator: In
                        values:
                          - "true"
          podAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: nvidia.com/dynamo-component-type
                    operator: In
                    values:
                    - worker
                topologyKey: kubernetes.io/hostname
          mainContainer:
            startupProbe:
              httpGet:
                path: /health
                port: 9090
              periodSeconds: 10
              timeoutSeconds: 10
              failureThreshold: 600
            env:
              - name: SERVED_MODEL_NAME
                value: "Qwen/Qwen3-8B"
              - name: MODEL_PATH
                value: "/opt/models/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218"
              - name: HF_HOME
                value: /opt/models
            args:
            - "python3 -m dynamo.vllm --model $MODEL_PATH --served-model-name $SERVED_MODEL_NAME --gpu-memory-utilization 0.90 --no-enable-prefix-caching --block-size 128 --is-decode-worker"
            command:
            - /bin/sh
            - -c
            image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.6.1
            workingDir: /workspace/examples/backends/vllm
    VllmPrefillWorker:
      dynamoNamespace: qwen3-8b-disagg-sn
      envFromSecret: hf-token
      componentType: worker
      subComponentType: prefill
      volumeMounts:
        - name: model-cache
          mountPoint: /opt/models
      sharedMemory:
        size: 80Gi
      replicas: 1
      resources:
        requests:
          cpu: "8"
          memory: "16Gi"
          gpu: "1"
        limits:
          cpu: "8"
          memory: "16Gi"
          gpu: "1"
      extraPodSpec:
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        affinity:
          nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: nvidia.com/gpu.present
                        operator: In
                        values:
                          - "true"
          podAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: nvidia.com/dynamo-component-type
                    operator: In
                    values:
                    - worker
                topologyKey: kubernetes.io/hostname
        mainContainer:
          startupProbe:
            httpGet:
              path: /health
              port: 9090
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 600
          env:
            - name: SERVED_MODEL_NAME
              value: "Qwen/Qwen3-8B"
            - name: MODEL_PATH
              value: "/opt/models/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218"
            - name: HF_HOME
              value: /opt/models
          args:
          - "python3 -m dynamo.vllm --model $MODEL_PATH --served-model-name $SERVED_MODEL_NAME --gpu-memory-utilization 0.90 --no-enable-prefix-caching --block-size 128 --is-prefix-worker"
          command:
          - /bin/sh
          - -c
          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.6.1
          workingDir: /workspace/examples/backends/vllm